# PREDICTIVE OBSERVATION BELIEF MODULE

This implementation has the observed action of the other agent added to the input of the prediction network, i.e. the prediction observation

# Multiple Channels

The previous implementation (NAME HERE) considered the case where Tx transmits on one channel and Rx receives on one channel, but they both select more channels to sense in an attempt to sense what channel the other agent choose in case they selected wrong.

What is new with this implementation is that Rx now selects multiple channels to receive on, and in addition to this it selects a few additional channels that it is just able to sense. The Tx however still only transmits on one, but is able to sense other channels in an attempt to detect Rx. If Rx is not able to decode the signal from Tx, it sends a "blip", i.e. a short burst, to make Tx aware of what it's most desirable channel was.

The difference between this and the previous FREQUENCY_HOPPING implementation is that this follows a set of sequences generated by pseduorandom number generators. The agents select a seed to follow for a specific amount of time.

The episode definition in this implementation is different. Each episode is defined as the transmission time. This means that the Tx and Rx receives a reward for every episode, but only updates after each NUM_HOPS episodes.

# Independent Deep Double Q-learning Networks (IDDQN) for Cooperative Multi-Agent Reinforcement Learning With Extended Action Space

This is a simple coordination task that extends on the GRU network and environment made in TFE4580 - "Anti-Jamming using Deep Q-learning".

The program in this directory extends on this to test the idea of cooperative multi-agent reinforcement learning. The agents are trained using Independent Deep Double Q-learning Networks (IDDQN) and are tested both for 10 channels and 100 channels. To maintain a successful communication link, the Rx must select the same band as the Tx. The Rx agent gets a reward if it is able to decode the message sent by the Tx agent. The Tx gets a reward if it is able to detect the ACK sent by the Rx.

Both the Tx and the Rx are only able to use observations that consist of NUM_SENSE_CHANNELS measurements, centered around the last chosen channel. The agents are trained using the same environment, but with different observations and actions. The Tx agent is trained to select the best channel to transmit on, while the Rx agent is trained to select the best channel to receive on. If a channel close to the first or last frequency band is selected, the agent will wrap around to the other side of the frequency band for the observations.

As an extension to the regular IDDQN implementation, the action selection is extended to include choosing channels to sense as well. The Tx will still select the best channel to transmit on and the Rx will still select the best channel to receive on. However, they will choose an additonal NUM_EXTRA_ACTIONS channels to sense. This means that even if the Rx chooses the wrong channel to receive on and will not be able to decode the transmitted signal, it can still sense which channel the Tx is transmitting on and learn from this. The same goes for the Tx, if it chooses the wrong channel to transmit on, it can still sense which channel the Rx is receiving on and learn from this.

This is a further extension to use the additional knowledge about the selected action to predict it in further time steps.